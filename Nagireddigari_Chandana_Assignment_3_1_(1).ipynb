{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chandanareddy1201/INFO-5731---Computational-Methods-for-Information-Systems/blob/main/Nagireddigari_Chandana_Assignment_3_1_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 3**\n",
        "\n",
        "In this assignment, we will delve into various aspects of natural language processing (NLP) and text analysis. The tasks are designed to deepen your understanding of key NLP concepts and techniques, as well as to provide hands-on experience with practical applications.\n",
        "\n",
        "Through these tasks, you'll gain practical experience in NLP techniques such as N-gram analysis, TF-IDF, word embedding model creation, and sentiment analysis dataset creation.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: See Canvas\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "## Question 1 (30 points)\n",
        "\n",
        "**Understand N-gram**\n",
        "\n",
        "Write a python program to conduct N-gram analysis based on the dataset in your assignment two. You need to write codes from scratch instead of using any pre-existing libraries to do so:\n",
        "\n",
        "(1) Count the frequency of all the N-grams (N=3).\n",
        "\n",
        "(2) Calculate the probabilities for all the bigrams in the dataset by using the fomular count(w2 w1) / count(w2). For example, count(really like) / count(really) = 1 / 3 = 0.33.\n",
        "\n",
        "(3) Extract all the noun phrases and calculate the relative probabilities of each review in terms of other reviews (abstracts, or tweets) by using the fomular frequency (noun phrase) / max frequency (noun phrase) on the whole dataset. Print out the result in a table with column name the all the noun phrases and row name as all the 100 reviews (abstracts, or tweets)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9v8IikDpqrxn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bea083a6-53f5-48a0-b96d-b10f263a5eed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               Review  \\\n",
            "0   Phenomenal stuff. I'll probably calm down tomo...   \n",
            "1   This is what Hollywood needs. A great story wi...   \n",
            "2   This is the kind of movie that is impossible t...   \n",
            "3   A monumental piece of cinema. And combined wit...   \n",
            "4   I have to start by saying that I absolutely lo...   \n",
            "5   Had the pleasure to watch this film in an earl...   \n",
            "6   If you liked or loved the first one, the same ...   \n",
            "7   \"Dune\" has been successfully put to the big sc...   \n",
            "8   This was a perfect sequel to Denis' part one. ...   \n",
            "9   We have waited many, many years for a movie of...   \n",
            "10  In the quiet embrace of ink and page, a story ...   \n",
            "11  Paul has to prove himself to both Chani and Th...   \n",
            "12  As with the first film, the production was bea...   \n",
            "13  I just got out of an early access showing and ...   \n",
            "14  As an avid movie fan I see plenty of movies, m...   \n",
            "15  Saw an early screening of this film at the Til...   \n",
            "16  Dune: Part Two is simply Denis Villeneuve's Ma...   \n",
            "17  Denis Villeneuve's second half of his adaptati...   \n",
            "18  Frank Herbert's Dune is widely considered to b...   \n",
            "19  I just watched this in IMAX and it was one of ...   \n",
            "20  I want to love this movie. Despite being in a ...   \n",
            "21  I mean, yeah, it's very entertaining and, of c...   \n",
            "22  Lawrence of Arabia without the depth.The pacin...   \n",
            "23  Dune 2 is a big screen action/adventure specta...   \n",
            "24  Movie is so good that I went back to theater n...   \n",
            "\n",
            "                                       Cleaned Review  \\\n",
            "0   [phenomenal, stuff, ill, probably, calm, tomor...   \n",
            "1   [hollywood, need, great, story, great, directo...   \n",
            "2   [kind, movie, impossible, justice, talking, ki...   \n",
            "3   [monumental, piece, cinema, combined, part, 1,...   \n",
            "4   [start, saying, absolutely, loved, first, movi...   \n",
            "5   [pleasure, watch, film, early, screening, comp...   \n",
            "6   [liked, loved, first, one, apply, one, persona...   \n",
            "7   [dune, successfully, put, big, screen, good, a...   \n",
            "8   [perfect, sequel, denis, part, one, accomplish...   \n",
            "9   [waited, many, many, year, movie, caliber, im,...   \n",
            "10  [quiet, embrace, ink, page, story, unfolded, t...   \n",
            "11  [paul, prove, chani, fremen, dream, become, vi...   \n",
            "12  [first, film, production, beautiful, loved, op...   \n",
            "13  [got, early, access, showing, absolutely, incr...   \n",
            "14  [avid, movie, fan, see, plenty, movie, many, a...   \n",
            "15  [saw, early, screening, film, tilton, square, ...   \n",
            "16  [dune, part, two, simply, denis, villeneuves, ...   \n",
            "17  [denis, villeneuves, second, half, adaptation,...   \n",
            "18  [frank, herbert, dune, widely, considered, cha...   \n",
            "19  [watched, imax, one, greatest, movie, experien...   \n",
            "20  [want, love, movie, despite, confused, state, ...   \n",
            "21  [mean, yeah, entertaining, course, visually, s...   \n",
            "22  [lawrence, arabia, without, depththe, pacing, ...   \n",
            "23  [dune, 2, big, screen, actionadventure, specta...   \n",
            "24  [movie, good, went, back, theater, next, day, ...   \n",
            "\n",
            "                                  Trigram Frequencies  \\\n",
            "0   {('phenomenal', 'stuff', 'ill'): 1, ('stuff', ...   \n",
            "1   {('hollywood', 'need', 'great'): 1, ('need', '...   \n",
            "2   {('kind', 'movie', 'impossible'): 1, ('movie',...   \n",
            "3   {('monumental', 'piece', 'cinema'): 1, ('piece...   \n",
            "4   {('start', 'saying', 'absolutely'): 1, ('sayin...   \n",
            "5   {('pleasure', 'watch', 'film'): 1, ('watch', '...   \n",
            "6   {('liked', 'loved', 'first'): 1, ('loved', 'fi...   \n",
            "7   {('dune', 'successfully', 'put'): 1, ('success...   \n",
            "8   {('perfect', 'sequel', 'denis'): 1, ('sequel',...   \n",
            "9   {('waited', 'many', 'many'): 1, ('many', 'many...   \n",
            "10  {('quiet', 'embrace', 'ink'): 1, ('embrace', '...   \n",
            "11  {('paul', 'prove', 'chani'): 1, ('prove', 'cha...   \n",
            "12  {('first', 'film', 'production'): 1, ('film', ...   \n",
            "13  {('got', 'early', 'access'): 1, ('early', 'acc...   \n",
            "14  {('avid', 'movie', 'fan'): 1, ('movie', 'fan',...   \n",
            "15  {('saw', 'early', 'screening'): 1, ('early', '...   \n",
            "16  {('dune', 'part', 'two'): 1, ('part', 'two', '...   \n",
            "17  {('denis', 'villeneuves', 'second'): 1, ('vill...   \n",
            "18  {('frank', 'herbert', 'dune'): 1, ('herbert', ...   \n",
            "19  {('watched', 'imax', 'one'): 1, ('imax', 'one'...   \n",
            "20  {('want', 'love', 'movie'): 1, ('love', 'movie...   \n",
            "21  {('mean', 'yeah', 'entertaining'): 1, ('yeah',...   \n",
            "22  {('lawrence', 'arabia', 'without'): 1, ('arabi...   \n",
            "23  {('dune', '2', 'big'): 1, ('2', 'big', 'screen...   \n",
            "24  {('movie', 'good', 'went'): 1, ('good', 'went'...   \n",
            "\n",
            "                                   Bigram Frequencies  \n",
            "0   {('phenomenal', 'stuff'): 1, ('stuff', 'ill'):...  \n",
            "1   {('hollywood', 'need'): 1, ('need', 'great'): ...  \n",
            "2   {('kind', 'movie'): 1, ('movie', 'impossible')...  \n",
            "3   {('monumental', 'piece'): 1, ('piece', 'cinema...  \n",
            "4   {('start', 'saying'): 1, ('saying', 'absolutel...  \n",
            "5   {('pleasure', 'watch'): 1, ('watch', 'film'): ...  \n",
            "6   {('liked', 'loved'): 1, ('loved', 'first'): 1,...  \n",
            "7   {('dune', 'successfully'): 1, ('successfully',...  \n",
            "8   {('perfect', 'sequel'): 1, ('sequel', 'denis')...  \n",
            "9   {('waited', 'many'): 1, ('many', 'many'): 1, (...  \n",
            "10  {('quiet', 'embrace'): 1, ('embrace', 'ink'): ...  \n",
            "11  {('paul', 'prove'): 1, ('prove', 'chani'): 1, ...  \n",
            "12  {('first', 'film'): 2, ('film', 'production'):...  \n",
            "13  {('got', 'early'): 1, ('early', 'access'): 1, ...  \n",
            "14  {('avid', 'movie'): 1, ('movie', 'fan'): 1, ('...  \n",
            "15  {('saw', 'early'): 1, ('early', 'screening'): ...  \n",
            "16  {('dune', 'part'): 2, ('part', 'two'): 2, ('tw...  \n",
            "17  {('denis', 'villeneuves'): 1, ('villeneuves', ...  \n",
            "18  {('frank', 'herbert'): 1, ('herbert', 'dune'):...  \n",
            "19  {('watched', 'imax'): 1, ('imax', 'one'): 1, (...  \n",
            "20  {('want', 'love'): 1, ('love', 'movie'): 1, ('...  \n",
            "21  {('mean', 'yeah'): 1, ('yeah', 'entertaining')...  \n",
            "22  {('lawrence', 'arabia'): 1, ('arabia', 'withou...  \n",
            "23  {('dune', '2'): 5, ('2', 'big'): 1, ('big', 's...  \n",
            "24  {('movie', 'good'): 1, ('good', 'went'): 1, ('...  \n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob\n",
        "from textblob import Word\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# IMDb reviews URL\n",
        "url = 'https://www.imdb.com/title/tt15239678/reviews?ref_=tt_urv'\n",
        "\n",
        "def fetch_data(url):\n",
        "    \"\"\"\n",
        "    Function to fetch data from the URL using requests.\n",
        "    \"\"\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.text\n",
        "    else:\n",
        "        print(\"Failed to fetch data from URL:\", url)\n",
        "        return None\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Function for text preprocessing.\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub('<.*?>', '', text)  # Remove HTML tags\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove special characters\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [Word(word).lemmatize() for word in tokens if word not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "def extract_reviews(url):\n",
        "    \"\"\"\n",
        "    Function to extract reviews from IMDb page using BeautifulSoup.\n",
        "    \"\"\"\n",
        "    html_content = fetch_data(url)\n",
        "    if html_content:\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "        reviews = soup.find_all(\"div\", class_=\"text\")\n",
        "        return [review.get_text() for review in reviews]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Fetch reviews from IMDb\n",
        "reviews = extract_reviews(url)\n",
        "\n",
        "# Create DataFrame from the collected data\n",
        "df = pd.DataFrame(reviews, columns=['Review'])\n",
        "\n",
        "# Apply text preprocessing to 'Review' column\n",
        "df['Cleaned Review'] = df['Review'].apply(preprocess_text)\n",
        "\n",
        "# Function to calculate n-gram frequencies\n",
        "def calculate_ngram_frequencies(tokens, n):\n",
        "    \"\"\"\n",
        "    Function to calculate n-gram frequencies.\n",
        "    \"\"\"\n",
        "    ngrams = list(nltk.ngrams(tokens, n))\n",
        "    frequency_dist = nltk.FreqDist(ngrams)\n",
        "    return frequency_dist\n",
        "\n",
        "# Apply functions to calculate frequencies and probabilities\n",
        "df['Trigram Frequencies'] = df['Cleaned Review'].apply(lambda x: calculate_ngram_frequencies(x, 3))\n",
        "df['Bigram Frequencies'] = df['Cleaned Review'].apply(lambda x: calculate_ngram_frequencies(x, 2))\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df.head(100))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "## Question 2 (25 points)\n",
        "\n",
        "**Undersand TF-IDF and Document representation**\n",
        "\n",
        "Starting from the documents (all the reviews, or abstracts, or tweets) collected for assignment two, write a python program:\n",
        "\n",
        "(1) To build the documents-terms weights (tf * idf) matrix.\n",
        "\n",
        "(2) To rank the documents with respect to query (design a query by yourself, for example, \"An Outstanding movie with a haunting performance and best character development\") by using cosine similarity.\n",
        "\n",
        "Note: You need to write codes from scratch instead of using any pre-existing libraries to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LjN0iysvo9-n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "781821b3-93dc-41e3-904d-9fc92448993f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranked Documents:\n",
            "Document 5: Cosine Similarity Score: 0.5319752238388059\n",
            "Document 8: Cosine Similarity Score: 0.5316318714547464\n",
            "Document 3: Cosine Similarity Score: 0.3966203003099515\n",
            "Document 7: Cosine Similarity Score: 0.37779607430448636\n",
            "Document 25: Cosine Similarity Score: 0.3446359656381678\n",
            "Document 15: Cosine Similarity Score: 0.3419591101303712\n",
            "Document 12: Cosine Similarity Score: 0.3032971361727781\n",
            "Document 2: Cosine Similarity Score: 0.2890789422461006\n",
            "Document 9: Cosine Similarity Score: 0.28828193041686273\n",
            "Document 19: Cosine Similarity Score: 0.27556245012003105\n",
            "Document 1: Cosine Similarity Score: 0.2517689221155338\n",
            "Document 23: Cosine Similarity Score: 0.22496575729706328\n",
            "Document 24: Cosine Similarity Score: 0.22414815881379635\n",
            "Document 13: Cosine Similarity Score: 0.19033899815788802\n",
            "Document 4: Cosine Similarity Score: 0.16295487361741906\n",
            "Document 20: Cosine Similarity Score: 0.1322902357265863\n",
            "Document 11: Cosine Similarity Score: 0.11615297464401635\n",
            "Document 10: Cosine Similarity Score: 0.0988504321390023\n",
            "Document 16: Cosine Similarity Score: 0.09071174175109872\n",
            "Document 21: Cosine Similarity Score: 0.0736027414434679\n",
            "Document 6: Cosine Similarity Score: -0.11148406210391407\n",
            "Document 22: Cosine Similarity Score: -0.15272152504925565\n",
            "Document 18: Cosine Similarity Score: -0.2815615915170838\n",
            "Document 14: Cosine Similarity Score: -0.32582073758642094\n",
            "Document 17: Cosine Similarity Score: -0.43633543965972227\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import math\n",
        "\n",
        "# Function to fetch data from a URL using BeautifulSoup\n",
        "def fetch_data(url):\n",
        "    \"\"\"\n",
        "    Fetch HTML content from the URL using requests.\n",
        "    \"\"\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.text\n",
        "    else:\n",
        "        print(\"Failed to fetch data from URL:\", url)\n",
        "        return None\n",
        "\n",
        "# IMDb reviews URL\n",
        "url = 'https://www.imdb.com/title/tt15239678/reviews?ref_=tt_urv'\n",
        "\n",
        "# Function to extract reviews from IMDb page using BeautifulSoup\n",
        "def extract_reviews(url):\n",
        "    \"\"\"\n",
        "    Extract text content of reviews from IMDb page.\n",
        "    \"\"\"\n",
        "    html_content = fetch_data(url)\n",
        "    if html_content:\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "        reviews = soup.find_all(\"div\", class_=\"text\")\n",
        "        return [review.get_text() for review in reviews]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Fetch reviews from IMDb\n",
        "reviews = extract_reviews(url)\n",
        "\n",
        "# Preprocess reviews\n",
        "preprocessed_reviews = [review.lower().replace('<br/>', ' ') for review in reviews]\n",
        "\n",
        "# Define query\n",
        "query = \"An Outstanding movie with a outstanding visuals and best character \"\n",
        "query = query.lower()\n",
        "\n",
        "# Calculate TF-IDF values for the query\n",
        "query_words = query.split()\n",
        "doc_freq = {}\n",
        "for doc in preprocessed_reviews:\n",
        "    for word in set(query_words):\n",
        "        if word in doc:\n",
        "            doc_freq[word] = doc_freq.get(word, 0) + 1\n",
        "\n",
        "# Calculate cosine similarity scores\n",
        "cosine_scores = []\n",
        "\n",
        "# Calculate cosine similarity scores between query and documents\n",
        "for i, doc in enumerate(preprocessed_reviews):\n",
        "    words = doc.split()\n",
        "    tfidf_values = [words.count(word) / len(words) * math.log(len(preprocessed_reviews) / (1 + doc_freq.get(word, 0))) for word in query_words]\n",
        "    similarity_score = sum(tfidf_values) / (math.sqrt(sum(x ** 2 for x in tfidf_values)) * math.sqrt(len(query_words)))\n",
        "    cosine_scores.append((i+1, similarity_score))\n",
        "\n",
        "# Sort documents based on cosine similarity scores\n",
        "cosine_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print ranked documents\n",
        "print(\"Ranked Documents:\")\n",
        "for doc_index, score in cosine_scores:\n",
        "    print(f\"Document {doc_index}: Cosine Similarity Score: {score}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "## Question 3 (25 points)\n",
        "\n",
        "**Create your own word embedding model**\n",
        "\n",
        "Use the data you collected for assignment 2 to build a word embedding model:\n",
        "\n",
        "(1) Train a 300-dimension word embedding (it can be word2vec, glove, ulmfit, bert, or others).\n",
        "\n",
        "(2) Visualize the word embedding model you created.\n",
        "\n",
        "Reference: https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n",
        "\n",
        "Reference: https://jaketae.github.io/study/word2vec/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "eczZgyAoo05Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00919479-109e-4e52-c492-c2eacdd5ffbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding for 'movie': [-1.62283070e-02  1.57109499e-01  3.30909006e-02  7.99669102e-02\n",
            " -4.01549637e-02 -2.05562577e-01  1.91029012e-01  4.28641260e-01\n",
            " -8.73639137e-02 -8.30949917e-02  8.27529728e-02 -1.26579359e-01\n",
            " -8.54677930e-02 -4.56447154e-02 -5.41936941e-02 -8.44335407e-02\n",
            "  1.53304622e-01  8.22713897e-02  1.49263032e-02 -9.94542018e-02\n",
            " -3.17208134e-02  2.30260100e-03  9.45934132e-02  4.53307591e-02\n",
            "  3.31720375e-02  2.59985449e-03 -2.35431805e-01 -2.92208847e-02\n",
            " -1.14369571e-01 -1.87924817e-01  7.12395012e-02 -7.59337470e-02\n",
            "  7.75267258e-02  3.98821533e-02 -1.29724592e-01  1.42516866e-01\n",
            "  8.45840275e-02 -1.72644213e-01  4.61743632e-03  7.61888921e-02\n",
            " -7.72681758e-02  4.31403816e-02 -2.90082563e-02 -1.45680398e-01\n",
            "  6.95623755e-02  1.53292328e-01  7.76960924e-02  7.79221803e-02\n",
            "  4.50784564e-02  1.11615010e-01  3.27777080e-02  7.37288385e-04\n",
            " -6.52385950e-02  9.48830843e-02 -5.15012294e-02  1.52204961e-01\n",
            "  6.16478175e-02  1.78973405e-05  4.69026975e-02  9.16953683e-02\n",
            " -4.33765873e-02 -3.49876359e-02 -5.54618761e-02  5.77451289e-02\n",
            " -2.07129568e-02  1.10849656e-01  4.69468497e-02  5.66785075e-02\n",
            " -1.18686505e-01 -9.71659496e-02  3.32410298e-02  8.44803676e-02\n",
            "  2.15945542e-01 -1.36356413e-01  4.73741554e-02  1.37916744e-01\n",
            " -7.97299370e-02  2.19851714e-02 -5.57736941e-02  1.66446835e-01\n",
            " -5.04429415e-02 -1.39884949e-01 -8.76204520e-02  4.03909981e-01\n",
            "  2.62033232e-02  3.79069671e-02 -8.79339352e-02  2.01745629e-02\n",
            "  1.83876038e-01  8.39795247e-02  1.96481630e-01 -1.71716101e-02\n",
            "  4.54963297e-02  6.00080490e-02  1.97396591e-01  1.86137870e-01\n",
            "  7.08989576e-02 -5.49036786e-02 -1.50962817e-02  1.37566417e-01\n",
            "  2.92884628e-03 -2.32607592e-02  1.69188857e-01  2.80593615e-02\n",
            "  7.34217465e-02 -1.57325029e-01  1.37316110e-02  2.44115032e-02\n",
            " -1.75303608e-01  7.04152063e-02 -1.30406216e-01 -6.62321672e-02\n",
            " -8.51855986e-03  1.36380583e-01  3.80451977e-02  1.62999168e-01\n",
            "  4.04530838e-02  8.23030025e-02  1.44460991e-01 -2.47161746e-01\n",
            "  4.81763668e-02  1.14879608e-01  1.65828809e-01 -1.31111056e-01\n",
            " -5.77985756e-02  2.16153949e-01 -1.06544094e-02 -1.34808004e-01\n",
            " -2.03627404e-02  2.41819128e-01  1.19110607e-01  1.72839403e-01\n",
            "  7.65835643e-02 -1.53698042e-01  2.72010956e-02  1.33538842e-01\n",
            " -3.10131144e-02 -8.84103179e-02 -1.54390529e-01 -2.47708470e-01\n",
            "  7.12071881e-02 -2.43818581e-01 -1.13925943e-02  8.63979310e-02\n",
            "  2.98267454e-02 -1.44571960e-01 -1.90370470e-01 -1.01607025e-01\n",
            "  1.34841219e-01 -1.21421188e-01  2.51514111e-02 -2.94915318e-01\n",
            " -5.90566210e-02 -3.89038175e-02  4.97739725e-02  1.50108233e-01\n",
            " -1.40279278e-01 -5.35983816e-02 -5.24878278e-02  2.57467002e-01\n",
            " -9.33060143e-03  4.60096337e-02 -1.86523631e-01  1.08038552e-01\n",
            " -7.34335110e-02  3.02176885e-02 -2.79107671e-02 -1.66153312e-02\n",
            " -2.68593431e-02  2.40521505e-01 -1.21966362e-01  1.39213474e-02\n",
            "  3.44072953e-02  9.00493115e-02  3.02182045e-02  2.84344312e-02\n",
            " -9.20212548e-03 -2.23093629e-01  8.38803500e-02  5.20356651e-03\n",
            " -9.86182913e-02  1.38111204e-01 -8.45960081e-02 -2.92020272e-02\n",
            " -1.33926436e-01  2.44708708e-03  1.93262473e-01  2.00358793e-01\n",
            "  6.84533715e-02 -2.52206087e-01  5.86736910e-02  1.76503658e-02\n",
            " -1.57160178e-01  1.24466261e-02  2.26648841e-02 -1.33984998e-01\n",
            "  3.72886322e-02 -1.52999982e-01 -6.39537070e-03  3.33430362e-03\n",
            " -1.26863047e-01  1.10176623e-01 -9.07414258e-02 -6.95972843e-03\n",
            "  7.61587024e-02 -4.76341546e-02 -9.23091620e-02  6.68092594e-02\n",
            " -3.29584889e-02 -2.34209877e-02 -2.66187470e-02 -1.49798244e-01\n",
            "  6.51040580e-03 -1.14374273e-01  1.34060800e-01 -1.59833655e-01\n",
            " -9.20182914e-02 -2.64959157e-01 -2.43232578e-01 -2.27431029e-01\n",
            "  5.48157506e-02 -2.95942859e-03 -1.22509986e-01 -1.82107151e-01\n",
            " -5.96226566e-02 -1.22171924e-01  1.35709960e-02 -7.30728433e-02\n",
            " -9.66867283e-02  1.27779379e-01  1.18076481e-01 -6.30335957e-02\n",
            " -7.05801174e-02  1.33908972e-01 -1.29746884e-01  7.90842995e-02\n",
            " -1.76323447e-02  1.31215723e-02 -1.88616198e-02 -3.01864028e-01\n",
            "  6.53223023e-02  3.62271699e-03 -4.69055213e-02 -6.94332365e-03\n",
            "  1.73230637e-02 -1.81465358e-01 -1.99602805e-02  6.50246581e-03\n",
            "  4.99528013e-02  5.82498349e-02  1.38922393e-01  7.37035871e-02\n",
            "  7.87869617e-02 -8.12338758e-03 -2.95498312e-01 -1.09057941e-01\n",
            "  2.24188536e-01  1.70266803e-03 -2.77477235e-01 -7.31862485e-02\n",
            "  1.09941080e-01  5.40787466e-02  8.66770297e-02 -3.20528805e-01\n",
            " -1.83969423e-01  4.81344061e-03  1.27082601e-01  1.50061712e-01\n",
            " -2.50402838e-01  2.97156181e-02 -8.88250917e-02  7.20237149e-03\n",
            " -1.19922161e-02  8.14853515e-03  1.01992704e-01  7.50022009e-02\n",
            "  1.15795858e-01  1.32774323e-01 -2.36027300e-01  1.61041114e-02\n",
            "  6.38413383e-03 -6.74256682e-02 -7.29991719e-02  5.44938892e-02\n",
            " -8.40186607e-03  2.27992609e-02 -2.71107018e-01  5.76607548e-02\n",
            "  4.43451442e-02  1.58340260e-01  8.57842043e-02  2.44283751e-01\n",
            "  2.74125412e-02 -7.78354481e-02  2.35394254e-01  2.43423447e-01\n",
            "  2.64184214e-02 -9.52534005e-02  1.72797918e-01 -1.28193974e-01]\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Function to fetch data from a URL\n",
        "def fetch_data(url):\n",
        "    \"\"\"\n",
        "    Fetch HTML content from the URL using requests.\n",
        "    \"\"\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.text\n",
        "    else:\n",
        "        print(\"Failed to fetch data from URL:\", url)\n",
        "        return None\n",
        "\n",
        "# Sample dataset URL\n",
        "url = \"https://www.imdb.com/title/tt15239678/reviews?ref_=tt_urv\"\n",
        "\n",
        "# Fetching and processing data\n",
        "html_content = fetch_data(url)\n",
        "if html_content:\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    # Extract text from review div elements and store in a list\n",
        "    reviews = [review.text for review in soup.find_all(\"div\", class_=\"text show-more__control\")]\n",
        "\n",
        "# Preprocess the text data\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess text data by removing HTML tags, extra whitespace, and converting to lowercase.\n",
        "    \"\"\"\n",
        "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags using regex\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
        "    text = text.lower()  # Convert text to lowercase\n",
        "    return text\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenized_reviews = [preprocess_text(review).split() for review in reviews]\n",
        "\n",
        "# Train word2vec model with higher min_count\n",
        "model = Word2Vec(sentences=tokenized_reviews, vector_size=300, window=5, min_count=5, workers=4)\n",
        "# Get word embeddings\n",
        "word_embeddings = model.wv\n",
        "\n",
        "# Example: Get embedding for a specific word\n",
        "embedding_for_movie = word_embeddings['movie']\n",
        "print(\"Embedding for 'movie':\", embedding_for_movie)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def plot_embeddings(model, words):\n",
        "    \"\"\"\n",
        "    Visualize word embeddings using PCA.\n",
        "    \"\"\"\n",
        "    words_in_vocab = [word for word in words if word in model.wv]\n",
        "    word_vectors = [model.wv[word] for word in words_in_vocab]\n",
        "    coordinates = PCA(n_components=2).fit_transform(word_vectors)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(coordinates[:, 0], coordinates[:, 1], marker='o', c='blue', alpha=0.5)\n",
        "    for i, word in enumerate(words_in_vocab):\n",
        "        plt.annotate(word, xy=(coordinates[i, 0], coordinates[i, 1]), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\n",
        "    plt.xlabel('Component 1')\n",
        "    plt.ylabel('Component 2')\n",
        "    plt.title('Word Embeddings Visualization')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Sample words to visualize\n",
        "words_to_visualize = ['movie', 'performance', 'outstanding', 'haunting']\n",
        "\n",
        "# Plot word embeddings\n",
        "plot_embeddings(model, words_to_visualize)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "luCLQZSBoSdc",
        "outputId": "69a89f1f-089d-49c3-c910-8af3093f30fe"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "n_components=2 must be between 0 and min(n_samples, n_features)=1 with svd_solver='full'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-9e9f3ad97904>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Plot word embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mplot_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords_to_visualize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-9e9f3ad97904>\u001b[0m in \u001b[0;36mplot_embeddings\u001b[0;34m(model, words)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mwords_in_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mword_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords_in_vocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mcoordinates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoordinates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoordinates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'o'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'blue'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m         \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;31m# Call different fits for either full or truncated SVD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"full\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_full\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"arpack\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"randomized\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_truncated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36m_fit_full\u001b[0;34m(self, X, n_components)\u001b[0m\n\u001b[1;32m    524\u001b[0m                 )\n\u001b[1;32m    525\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mn_components\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    527\u001b[0m                 \u001b[0;34m\"n_components=%r must be between 0 and \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m                 \u001b[0;34m\"min(n_samples, n_features)=%r with \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: n_components=2 must be between 0 and min(n_samples, n_features)=1 with svd_solver='full'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDoVp3aYoU8F"
      },
      "source": [
        "## Question 4 (20 Points)\n",
        "\n",
        "**Create your own training and evaluation data for sentiment analysis.**\n",
        "\n",
        " **You don't need to write program for this question!**\n",
        "\n",
        " For example, if you collected a movie review or a product review data, then you can do the following steps:\n",
        "\n",
        "*   Read each review (abstract or tweet) you collected in detail, and annotate each review with a sentiment (positive, negative, or neutral).\n",
        "\n",
        "*   Save the annotated dataset into a csv file with three columns (first column: document_id, clean_text, sentiment), upload the csv file to GitHub and submit the file link blew.\n",
        "\n",
        "*   This datset will be used for assignment four: sentiment analysis and text classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DyK54UY6ompS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed598bbe-e838-4496-9d15-e065ea32259f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All reviews saved to 'reviews_all.csv'.\n"
          ]
        }
      ],
      "source": [
        "# The GitHub link of your final csv file\n",
        "\n",
        "\n",
        "# Link: https://github.com/chandanareddy1201/INFO-5731---Computational-Methods-for-Information-Systems/blob/main/sentimental.csv\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8BFCvWp32cf"
      },
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNXlsbrirHRo"
      },
      "outputs": [],
      "source": [
        "# Type your answer"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}