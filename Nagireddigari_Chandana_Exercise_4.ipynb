{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 4**\n",
        "\n",
        "**This exercise will provide a valuable learning experience in working with text data and extracting features using various topic modeling algorithms. Key concepts such as Latent Dirichlet Allocation (LDA), Latent Semantic Analysis (LSA), lda2vec, and BERTopic.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks***.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission, and no requests will be answered. Manage your time accordingly.**\n"
      ],
      "metadata": {
        "id": "TU-pLW33lpcS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "\n",
        "**Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "VAZj4PHB70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a04e57b1-a237-4fe5-efc9-d2ac6da67e8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal number of topics: 7\n",
            "Topic 0: 0.334*\"bottle\" + 0.333*\"versace\" + 0.333*\"cologne\"\n",
            "Topic 1: 0.489*\"bottle\" + 0.333*\"versace\" + 0.178*\"cologne\"\n",
            "Topic 2: 0.880*\"bottle\" + 0.060*\"cologne\" + 0.060*\"versace\"\n",
            "Topic 3: 0.763*\"cologne\" + 0.211*\"versace\" + 0.027*\"bottle\"\n",
            "Topic 4: 0.334*\"bottle\" + 0.334*\"versace\" + 0.332*\"cologne\"\n",
            "Topic 5: 0.881*\"versace\" + 0.060*\"bottle\" + 0.060*\"cologne\"\n",
            "Topic 6: 0.496*\"versace\" + 0.442*\"cologne\" + 0.062*\"bottle\"\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "import pandas as pd\n",
        "from gensim.models import LdaModel\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "# Download NLTK stopwords corpus\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the data from the CSV file\n",
        "df = pd.read_csv('extract.csv')\n",
        "data = df['Review'].tolist()  # Replace 'your_text_column_name' with the actual column name\n",
        "\n",
        "# Preprocess the text\n",
        "stop_words = set(stopwords.words('english'))\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "texts = [[word for word in word_tokenize(str(doc).lower()) if word.isalpha() and word not in stop_words] for doc in data]\n",
        "\n",
        "# Create a dictionary representation of the documents\n",
        "dictionary = Dictionary(texts)\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.5)  # Filter out too rare or too common words\n",
        "\n",
        "# Create a document-term matrix\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "# Iterate over different values of K\n",
        "best_coherence = -1\n",
        "best_lda_model = None\n",
        "best_k = 0\n",
        "for k in range(2, 11):\n",
        "    # Train the LDA model\n",
        "    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=k, random_state=42)\n",
        "\n",
        "    # Compute the coherence score\n",
        "    coherence_model = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "\n",
        "    # Update the best model and coherence score\n",
        "    if coherence_score > best_coherence:\n",
        "        best_coherence = coherence_score\n",
        "        best_lda_model = lda_model\n",
        "        best_k = k\n",
        "\n",
        "# Print the optimal number of topics\n",
        "print(\"Optimal number of topics:\", best_k)\n",
        "\n",
        "# Summarize the topics\n",
        "for idx, topic in best_lda_model.print_topics(-1):\n",
        "    print(\"Topic {}: {}\".format(idx, topic))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "\n",
        "**Generate K topics by using LSA, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31c9e76a-845e-4a38-b0bb-0eb845909793"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal number of topics: 10\n",
            "Topic 0: 0.760*\"versace\" + 0.638*\"cologne\" + 0.125*\"bottle\"\n",
            "Topic 1: 0.723*\"cologne\" + -0.536*\"versace\" + -0.436*\"bottle\"\n",
            "Topic 2: -0.891*\"bottle\" + 0.368*\"versace\" + -0.264*\"cologne\"\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from gensim.models import LsiModel\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "# Load the data from the CSV file\n",
        "df = pd.read_csv('extract.csv')\n",
        "data = df['Review'].tolist()  # Replace 'text_column' with the actual column name containing the text data\n",
        "\n",
        "# Preprocess the text\n",
        "stop_words = set(stopwords.words('english'))\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "texts = [[word for word in word_tokenize(str(doc).lower()) if word.isalpha() and word not in stop_words] for doc in data]\n",
        "\n",
        "# Create a dictionary representation of the documents\n",
        "dictionary = Dictionary(texts)\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.5)  # Filter out too rare or too common words\n",
        "\n",
        "# Create a document-term matrix\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "# Iterate over different values of K\n",
        "best_coherence = -1\n",
        "best_lsa_model = None\n",
        "best_k = 0\n",
        "for k in range(2, 11):\n",
        "    # Train the LSA model\n",
        "    lsa_model = LsiModel(corpus=corpus, id2word=dictionary, num_topics=k)\n",
        "\n",
        "    # Compute the coherence score\n",
        "    coherence_model = CoherenceModel(model=lsa_model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "\n",
        "    # Update the best model and coherence score\n",
        "    if coherence_score > best_coherence:\n",
        "        best_coherence = coherence_score\n",
        "        best_lsa_model = lsa_model\n",
        "        best_k = k\n",
        "\n",
        "# Print the optimal number of topics\n",
        "print(\"Optimal number of topics:\", best_k)\n",
        "\n",
        "# Summarize the topics\n",
        "for idx, topic in best_lsa_model.print_topics(-1):\n",
        "    print(\"Topic {}: {}\".format(idx, topic))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "**Generate K topics by using lda2vec, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://nbviewer.org/github/cemoody/lda2vec/blob/master/examples/twenty_newsgroups/lda2vec/lda2vec.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04f00ea0-c7b7-4bb5-93e3-6ee04b4fc858"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal number of topics: 7\n",
            "Topic 0: 0.334*\"bottle\" + 0.333*\"versace\" + 0.333*\"cologne\"\n",
            "Topic 1: 0.489*\"bottle\" + 0.333*\"versace\" + 0.178*\"cologne\"\n",
            "Topic 2: 0.880*\"bottle\" + 0.060*\"cologne\" + 0.060*\"versace\"\n",
            "Topic 3: 0.763*\"cologne\" + 0.211*\"versace\" + 0.027*\"bottle\"\n",
            "Topic 4: 0.334*\"bottle\" + 0.334*\"versace\" + 0.332*\"cologne\"\n",
            "Topic 5: 0.881*\"versace\" + 0.060*\"bottle\" + 0.060*\"cologne\"\n",
            "Topic 6: 0.496*\"versace\" + 0.442*\"cologne\" + 0.062*\"bottle\"\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from gensim.models import LdaModel\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "# Load the data from the CSV file\n",
        "df = pd.read_csv('extract.csv')\n",
        "data = df['Review'].tolist()\n",
        "\n",
        "# Preprocess the text\n",
        "stop_words = set(stopwords.words('english'))\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "texts = [[word for word in word_tokenize(str(doc).lower()) if word.isalpha() and word not in stop_words] for doc in data]\n",
        "\n",
        "# Create a dictionary representation of the documents\n",
        "dictionary = Dictionary(texts)\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.5)  # Filter out too rare or too common words\n",
        "\n",
        "# Create a document-term matrix\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "# Iterate over different values of K\n",
        "best_coherence = -1\n",
        "best_lda_model = None\n",
        "best_k = 0\n",
        "for k in range(2, 11):\n",
        "    # Train the LDA model\n",
        "    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=k, random_state=42)\n",
        "\n",
        "    # Compute the coherence score\n",
        "    coherence_model = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "\n",
        "    # Update the best model and coherence score\n",
        "    if coherence_score > best_coherence:\n",
        "        best_coherence = coherence_score\n",
        "        best_lda_model = lda_model\n",
        "        best_k = k\n",
        "\n",
        "# Print the optimal number of topics\n",
        "print(\"Optimal number of topics:\", best_k)\n",
        "\n",
        "# Summarize the topics\n",
        "for idx, topic in best_lda_model.print_topics(-1):\n",
        "    print(\"Topic {}: {}\".format(idx, topic))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "**Generate K topics by using BERTopic, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://colab.research.google.com/drive/1FieRA9fLdkQEGDIMYl0I3MCjSUKVF8C-?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6b30841-419e-4ba9-fa54-1040cc4f6f53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0: -1\n",
            "Topic 1: -1\n",
            "Topic 2: -1\n",
            "Topic 3: -1\n",
            "Topic 4: -1\n",
            "Topic 5: -1\n",
            "Topic 6: -1\n",
            "Topic 7: -1\n",
            "Topic 8: -1\n",
            "Topic 9: -1\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from bertopic import BERTopic\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "# Load the data from the CSV file\n",
        "df = pd.read_csv('extract.csv')\n",
        "data = df['Review'].tolist()\n",
        "\n",
        "# Preprocess the text\n",
        "stop_words = set(stopwords.words('english'))\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "texts = [' '.join([word for word in word_tokenize(str(doc).lower()) if word.isalpha() and word not in stop_words]) for doc in data]\n",
        "\n",
        "# Initialize BERTopic\n",
        "bertopic_model = BERTopic()\n",
        "\n",
        "# Fit BERTopic on the preprocessed text\n",
        "topics, _ = bertopic_model.fit_transform(texts)\n",
        "\n",
        "# Print the topics\n",
        "for topic_id, words in enumerate(topics):\n",
        "    print(f\"Topic {topic_id}: {words}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra Question (5 Points)\n",
        "\n",
        "**Compare the results generated by the four topic modeling algorithms, which one is better? You should explain the reasons in details.**\n",
        "\n",
        "**This question will compensate for any points deducted in this exercise. Maximum marks for the exercise is 40 points.**"
      ],
      "metadata": {
        "id": "d89ODUx3jjJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "The topic modeling algorithms LDA, LSA, lda2vec, and BERTopic all come with their OWN strengths and short comings, therefore, the choice on which one to apply for a particular use case depends solely on the requirements of the user or organization.\n",
        " LDA and LSA are well-known for the topics they give out as it is possible to understand these topics even without assistance which is a feature-driven task. Alternatively, they can face difficulties in the scalability of their models to the big data. Instead,\n",
        "  lda2vec and BERTopic provide for greater flexibility and context considerations but may at the same time deliver less explicit contents.\n",
        "\n",
        "\n",
        "When you look at the amount of accuracy, the coherence scores can be applied as the measure, with higher values for higher topics quality. LDA and LSA are often easier to understand,\n",
        " so they are very suitable for those who just start the work, while lda2vec and BERTopic can be more technical, so they're better for the people who have more knowledge in the field.\n",
        "  Furthermore, there is a variety of algorithms brought by the requirements of a particular task. Some of these algorithms are with respect to interpretability, scalability, performance, flexibility and reliability.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "g2DO8VjMoKBN",
        "outputId": "3b36dfba-6802-4a71-ba47-e1e3227c2002"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nThe topic modeling algorithms LDA, LSA, lda2vec, and BERTopic all come with their OWN strengths and short comings, therefore, the choice on which one to apply for a particular use case depends solely on the requirements of the user or organization. LDA and LSA are well-known for the topics they give out as it is possible to understand these topics even without assistance which is a feature-driven task. Alternatively, they can face difficulties in the scalability of their models to the big data. Instead, lda2vec and BERTopic provide for greater flexibility and context considerations but may at the same time deliver less explicit contents.\\n\\n\\nWhen you look at the amount of accuracy, the coherence scores can be applied as the measure, with higher values for higher topics quality. LDA and LSA are often easier to understand, so they are very suitable for those who just start the work, while lda2vec and BERTopic can be more technical, so they're better for the people who have more knowledge in the field. Furthermore, there is a variety of algorithms brought by the requirements of a particular task. Some of these algorithms are with respect to interpretability, scalability, performance, flexibility and reliability.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Fe_oh-5inUrA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment.\n",
        "\n",
        "Consider the following points in your response:\n",
        "\n",
        "**Learning Experience:** Describe your overall learning experience in working with text data and extracting features using various topic modeling algorithms. Did you understand these algorithms and did the implementations helped in grasping the nuances of feature extraction from text data.\n",
        "\n",
        "**Challenges Encountered:** Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "This task has become a great hands-on approach to NLP, engaging in the text data and getting features through the topic modeling algorithms. Thoroughly analyzing LDA, LSA, lda2vec and BERTopic provided me with a deeper comprehension of these algorithms and their properties that help in text feature extraction. Challenges of it were implementation of very complicated algorithms like lda2vec ,and BERTopic, and handling of large data sets in an appropriate manner. It is especially with NLP in mind, as the aim here is to build fundamental tasks like topic modeling and gain insights about a subject. Finally, this assignment represents an initial step into NLP research and application by including the very fundamentals of text data processing and feature extraction.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}