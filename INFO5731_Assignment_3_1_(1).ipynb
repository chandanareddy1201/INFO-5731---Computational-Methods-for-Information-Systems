{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chandanareddy1201/INFO-5731---Computational-Methods-for-Information-Systems/blob/main/INFO5731_Assignment_3_1_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 3**\n",
        "\n",
        "In this assignment, we will delve into various aspects of natural language processing (NLP) and text analysis. The tasks are designed to deepen your understanding of key NLP concepts and techniques, as well as to provide hands-on experience with practical applications.\n",
        "\n",
        "Through these tasks, you'll gain practical experience in NLP techniques such as N-gram analysis, TF-IDF, word embedding model creation, and sentiment analysis dataset creation.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: See Canvas\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "## Question 1 (30 points)\n",
        "\n",
        "**Understand N-gram**\n",
        "\n",
        "Write a python program to conduct N-gram analysis based on the dataset in your assignment two. You need to write codes from scratch instead of using any pre-existing libraries to do so:\n",
        "\n",
        "(1) Count the frequency of all the N-grams (N=3).\n",
        "\n",
        "(2) Calculate the probabilities for all the bigrams in the dataset by using the fomular count(w2 w1) / count(w2). For example, count(really like) / count(really) = 1 / 3 = 0.33.\n",
        "\n",
        "(3) Extract all the noun phrases and calculate the relative probabilities of each review in terms of other reviews (abstracts, or tweets) by using the fomular frequency (noun phrase) / max frequency (noun phrase) on the whole dataset. Print out the result in a table with column name the all the noun phrases and row name as all the 100 reviews (abstracts, or tweets)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9v8IikDpqrxn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "390ef3af-be7d-4932-de0c-b2653f10d92d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch data from URL: https://ddr.densho.org/narrators/\n",
            "No data fetched from the URL.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "# Function to fetch data from URL\n",
        "def fetch_data(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.text\n",
        "    else:\n",
        "        print(\"Failed to fetch data from URL:\", url)\n",
        "        return None\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation and special characters\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "# Function to generate N-grams\n",
        "def generate_ngrams(tokens, n):\n",
        "    ngrams = []\n",
        "    for i in range(len(tokens)-n+1):\n",
        "        ngrams.append(tuple(tokens[i:i+n]))\n",
        "    return ngrams\n",
        "\n",
        "# Function to count frequencies of N-grams\n",
        "def count_ngram_frequencies(texts, n):\n",
        "    ngram_freq = defaultdict(int)\n",
        "    total_ngrams = 0\n",
        "    for text in texts:\n",
        "        tokens = text.split()\n",
        "        ngrams = generate_ngrams(tokens, n)\n",
        "        for ngram in ngrams:\n",
        "            ngram_freq[ngram] += 1\n",
        "            total_ngrams += 1\n",
        "    return ngram_freq, total_ngrams\n",
        "\n",
        "# Function to extract noun phrases\n",
        "def extract_noun_phrases(text):\n",
        "    noun_phrases = re.findall(r'\\b(?:NN(?:P|PS)?|JJ)*(?:\\s+\\b(?:NN(?:P|PS)?|JJ)+)*\\b', text)\n",
        "    return noun_phrases\n",
        "\n",
        "# Function to calculate relative probabilities of noun phrases\n",
        "def calculate_relative_probabilities(texts):\n",
        "    noun_phrase_freq = defaultdict(int)\n",
        "    max_freq = 0\n",
        "    for text in texts:\n",
        "        noun_phrases = extract_noun_phrases(text)\n",
        "        for phrase in noun_phrases:\n",
        "            noun_phrase_freq[phrase] += 1\n",
        "            if noun_phrase_freq[phrase] > max_freq:\n",
        "                max_freq = noun_phrase_freq[phrase]\n",
        "    relative_probabilities = {}\n",
        "    for i, text in enumerate(texts, 1):\n",
        "        relative_probabilities[i] = {}\n",
        "        noun_phrases = extract_noun_phrases(text)\n",
        "        for phrase in noun_phrases:\n",
        "            relative_probabilities[i][phrase] = noun_phrase_freq[phrase] / max_freq\n",
        "    return relative_probabilities\n",
        "\n",
        "# URL to fetch data\n",
        "url = \"https://ddr.densho.org/narrators/\"\n",
        "# Fetch data from URL\n",
        "data = fetch_data(url)\n",
        "\n",
        "if data:\n",
        "    # Parse HTML content\n",
        "    soup = BeautifulSoup(data, 'html.parser')\n",
        "    # Extract text from HTML\n",
        "    text = soup.get_text()\n",
        "    # Preprocess the text\n",
        "    preprocessed_text = preprocess_text(text)\n",
        "\n",
        "    # Perform N-gram analysis\n",
        "    n = 3  # N for N-grams (change as needed)\n",
        "    trigram_freq, total_trigrams = count_ngram_frequencies([preprocessed_text], n)\n",
        "    print(\"Trigram Frequencies:\")\n",
        "    for trigram, freq in trigram_freq.items():\n",
        "        print(trigram, \":\", freq)\n",
        "\n",
        "    # Extract noun phrases\n",
        "    noun_phrases = extract_noun_phrases(preprocessed_text)\n",
        "\n",
        "    # Calculate relative probabilities of noun phrases\n",
        "    relative_probabilities = calculate_relative_probabilities([preprocessed_text])\n",
        "    print(\"\\nRelative Probabilities of Noun Phrases:\")\n",
        "    for i, probs in relative_probabilities.items():\n",
        "        print(\"Review\", i, \":\")\n",
        "        for phrase, rel_prob in probs.items():\n",
        "            print(phrase, \":\", rel_prob)\n",
        "else:\n",
        "    print(\"No data fetched from the URL.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "## Question 2 (25 points)\n",
        "\n",
        "**Undersand TF-IDF and Document representation**\n",
        "\n",
        "Starting from the documents (all the reviews, or abstracts, or tweets) collected for assignment two, write a python program:\n",
        "\n",
        "(1) To build the documents-terms weights (tf * idf) matrix.\n",
        "\n",
        "(2) To rank the documents with respect to query (design a query by yourself, for example, \"An Outstanding movie with a haunting performance and best character development\") by using cosine similarity.\n",
        "\n",
        "Note: You need to write codes from scratch instead of using any pre-existing libraries to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LjN0iysvo9-n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "7bbdedec-24c0-454b-f732-c61069e048c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch data from URL: https://ddr.densho.org/narrators/\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'NoneType' object is not iterable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-f326131e643b>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# Preprocess reviews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mpreprocessed_reviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<br/>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreviews\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Define query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "## Question 3 (25 points)\n",
        "\n",
        "**Create your own word embedding model**\n",
        "\n",
        "Use the data you collected for assignment 2 to build a word embedding model:\n",
        "\n",
        "(1) Train a 300-dimension word embedding (it can be word2vec, glove, ulmfit, bert, or others).\n",
        "\n",
        "(2) Visualize the word embedding model you created.\n",
        "\n",
        "Reference: https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n",
        "\n",
        "Reference: https://jaketae.github.io/study/word2vec/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eczZgyAoo05Q"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDoVp3aYoU8F"
      },
      "source": [
        "## Question 4 (20 Points)\n",
        "\n",
        "**Create your own training and evaluation data for sentiment analysis.**\n",
        "\n",
        " **You don't need to write program for this question!**\n",
        "\n",
        " For example, if you collected a movie review or a product review data, then you can do the following steps:\n",
        "\n",
        "*   Read each review (abstract or tweet) you collected in detail, and annotate each review with a sentiment (positive, negative, or neutral).\n",
        "\n",
        "*   Save the annotated dataset into a csv file with three columns (first column: document_id, clean_text, sentiment), upload the csv file to GitHub and submit the file link blew.\n",
        "\n",
        "*   This datset will be used for assignment four: sentiment analysis and text classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DyK54UY6ompS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed598bbe-e838-4496-9d15-e065ea32259f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All reviews saved to 'reviews_all.csv'.\n"
          ]
        }
      ],
      "source": [
        "# The GitHub link of your final csv file\n",
        "\n",
        "\n",
        "# Link:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8BFCvWp32cf"
      },
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNXlsbrirHRo"
      },
      "outputs": [],
      "source": [
        "# Type your answer"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}